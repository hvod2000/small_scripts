#!/usr/bin/env python
r"""
Usage:
    compile-snippets <source>
    compile-snippets -h | --help

Options:
    -h --help        Show this screen.

Examples:
    compile-snippets hello_world.py
"""
from string import punctuation, whitespace
from pathlib import Path

MAGIC_NAMES = {"sharp": "#", "shebang": "#!"}


def str_to_json(string: str) -> str:
    escaping = {"\\": "\\\\", '"': '\\"'}
    return '"' + "".join(escaping.get(char, char) for char in string) + '"'


def tokenize(source):
    i = 0
    while i < len(source):
        if source[i] in "'\"":
            j = i + 1
            while j < len(source) and not (source[j] == source[i] and source[j - 1] != '\\'):
                j += 1
            yield source[i:j + 1]
            i = j + 1
        elif source[i] in punctuation:
            yield source[i]
            i += 1
        elif source[i] in whitespace:
            j = i + 1
            while j < len(source) and source[j] in whitespace:
                j += 1
            yield source[i:j]
            i = j
        elif source[i].isdecimal():
            j = i + 1
            while j < len(source) and source[i:j + 1].isdecimal():
                j += 1
            yield source[i:j]
            i = j
        else:
            j = i + 1
            while j < len(source) and source[j] not in punctuation + whitespace:
                j += 1
            yield source[i:j]
            i = j


def parse_snippets(source: str):
    i, lines = 0, list(source.split("\n"))
    global_substitutions = {}
    while i < len(lines):
        tokens = list(tokenize(lines[i]))
        if tokens[:2] != ["def", " "]:
            if tokens[1:4] == [" ", "=", " "]:
                global_substitutions[tokens[0]] = tokens[4].strip("'").strip('"')
            i += 1
            continue
        name = MAGIC_NAMES.get(tokens[2], tokens[2])
        args = list(zip(tokens[4::5], tokens[6::5]))
        j = i + 1
        while j < len(lines) and (lines[j].startswith("    ") or not lines[j]):
            j += 1
        variables = [(var, default.strip("'").strip('"')) for var, default in args]
        snippet_lines = [
            "".join(global_substitutions.get(token, token) for token in tokenize(line))
            for line in lines[i + 1: j]
        ]
        while len(snippet_lines) > 1 and snippet_lines[-1] == "":
            snippet_lines.pop()
        yield (name, variables, snippet_lines)
        i = j


def snippet_to_json(name: str, variables: list[tuple[str, str]], source: list[str]) -> str:
    result = []
    result.append(f'"{name}": ' + "{")
    result.append(f'  "prefix": "{name}",')
    result.append('  "body": [')
    lookup = {
        var: "${" + str(i) + ":" + default + "}"
        for i, (var, default) in enumerate(variables, 1)
    }
    for tokens in map(tokenize, source):
        line = "".join(lookup.get(token, token) for token in tokens)
        result.append(f"    {str_to_json(line)},")
    result[-1] = result[-1].removesuffix(",")
    result.append("  ]")
    result.append("},")
    return "\n".join(result)


def main(source_path: str):
    lines = []
    for snippet in parse_snippets(Path(source_path).read_text()):
        lines.extend(snippet_to_json(*snippet).split("\n"))
    if lines:
        lines[-1] = lines[-1].removesuffix(",")
    print("{")
    for line in lines:
        print(" ", line)
    print("}")


if __name__ == "__main__":
    args = __import__("docopt").docopt(__doc__)
    main(source_path=args["<source>"])
